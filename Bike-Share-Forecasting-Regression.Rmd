---
title: "Midterm"
output:
  pdf_document: default
  html_document: default
---

Background
Bike sharing systems are new generation of traditional bike rentals where the whole process from membership, rental to return-back has become automatic. Through these systems, the user is able to easily rent a bike from a particular position and return back at another position. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns the bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.
Bike-sharing rental process is highly correlated with the environmental and seasonal settings. For instance, weather conditions, precipitation, day of week, season, hour of the day, etc., can affect the rental behaviors. The dataset is related to the two-year historical log (aggregated on daily basis) corresponding to years 2011 and 2012 from the Capital Bikeshare system, Washington D.C., USA. The variables are defined as follows:

season 1=spring, 2=summer, 3=fall, 4=winter
year 0=2011, 1=2012
month 1=Jan, 2=Feb, .., 12=Dec
holiday whether the day is holiday or not (1=holiday, 0=not)
weekday day of the week (0=Sun, 1=Mon, .., 6=Sat)
weathersit weather condition of the day
1= Clear, Few clouds, Partly cloudy
2= Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
3= Light Snow, Light rain + Thunderstorm + Scattered clouds
4= Heavy Rain + Ice pallets + Thunderstorm + Mist, Snow + Fog)
temp normalized temperature in Celsius
atemp normalized feeling temperature in Celsius
hum normalized humidity
windspeed normalized wind speed
casual count of casual users (response)


I. Training
Using the training dataset (Training.csv) in this folder, perform the following analysis:

1. Develop a regression model to predict casual using all other variables (consider only main effects of predictors).

```{r}
setwd("D:/TAMU/Courses/Spring2017/ISEN 613/Midterm I")   #Change root directory
train = read.csv("training.csv",header = TRUE)           #Read training data from new path
str(train)                                               #Get quick over view of datset
lm.fit1 <- lm(casual ~ .,data = train)                   #Fit LR model against all variables
```

Interpret the fitted model. Is this model good for the data? Why?

```{r}
summary(lm.fit1)                                         #Get model summary
```

The summary function provides us a list of all coefficients for all predictors and the signifance of each predictor in terms of contributing to the model fit.

Key takeaways from Summary table :
a) Overall, the p-value from the F-statistic should be checked first which signifies if atleast one of our variables is correlated with our response variable. The p-value is very low, which makes us reject the null hypothesis which states there is no relation between any of the predictor variables with our response.

Predictors Weathersit and year has the highest significance in predicting the model, judging by its corresponding p-values. Holiday, weekday and windspeed are also significant variables in our model, even if not as significant as the previous two. Season can also be considered significant as it is marginally inside the critical t-value. 



b) In terms of judging if the model is good for our data, we must say it is fairly poor. The statistic which guides us here is the adjusted R squared value which comes out to be 0.3844. This means the model is only capable of explaining about 38 % of the variability present in the data.

3. Conduct model diagnostics and discuss how you plan to solve the detected problems, if any.

First of all, its always important and good to plot our data set to see if there are any discernible trends present within the data before undertaking any advanced techniques.

```{r}
par(mfrow = c(2,2))
pairs(~.,data = train)                               #Obtain scatter plots for each variables
```

We are primarily interested in the relationship between our response variable - casual and the predictor variables and if there are any obvious correlations between any of these to the response.

We see certain patterns in the data from these plots. Eg : Casual has a positive correlation with temperature. Also, we see a peak in people renting bikes in the middle of the season (between early January and early Winter). Similarly with month - Peaking during the middle of the year.

Also, there is a rush of bike renters over the weekends as compared to weekdays. All these are also fairly logical and we can come back to these relations for modifying our model later on.


Model Diagnostics : 

```{r}
par(mfrow = c(2, 2))
plot(lm.fit1)                                     # Get model diagnostics for our first model
```

We perform model diagnostics to assess if there are any problems withn our model.
The residuals vs predicted values plot is definitely not random and you can see the variance is increasing as the value of y increases. (Ideally it should spread evenly around zero and have constant variance.)

Also, in the Normal Q-Q plot, there is a variation from the straight line relation which indicates the residuals are approximately normally distributed. 

There are no leverage and outlier points which are causing us any major concerns.

Possible solution ?

From the funnel shape variance curve seen from the Residuals vs Fitted values chart, we can possibly try two methods : 
a) Weighted Least Squares(WLS) method
b) Transformation of response variable

We choose transformation of response variable and see the possible effects.

Generally regression techniques tend to do well when data is symmetrically distributed/assume a bell shaped curve.

Plotting our response variable casual, we see it is highly skewed :
```{r}
hist(train$casual, col = "blue")
```

Now, if we plot the data after taking the histogram of the log transformed response, we get something like this

```{r}
hist(log10(train$casual),col = "red")
```
This is much more in line with what we are looking for. 
Now the best way to see if if it works would be to use this to build a new model and check corresponding model metrics and diagnostic plots to substantiate this claim.

4. Improve the model by adding or deleting variables, adding interactions, and adding nonlinear transformation of variables, etc. Choose two models as final models (clearly indicate the chosen models).

Before creating the new log transformed variable , it is worthwhile to see the correlation between our existing variables in the train dataset 
```{r}
cor(train)                           #Obtain correlation between all variables within dataset
```

There is a decent positive correlation between casual response and temperature variable.
There is also understandable positve correlation between humidity and the seasons (and consequently month)
Season and temp are also moderately correlated in a positive direction.

```{r}
train$casuallog = log10(train$casual)        #Transforming response - log transform
```

Improvement Models :

Model I:
Once we have created the new transformed response variable, we will build a new LR model using almost all variables.(Removed atemp as it is redundant and conveys same information as temp)

```{r}
lm.fit2 <- lm(casuallog ~ year+season+temp+holiday+weekday+weathersit+month+hum+windspeed, data = train)
summary(lm.fit2)
```

Analyzing the results, we see the model has significant factors and also, the adjusted R squared value, which provides a sense of the accuracy of the model, shoots upto to nearly 57%.

Now we will analyze the diagnostic plots again for this model and see if we are justified in transforming the response variables from our initial assumptions.

```{r}
par(mfrow = c(2,2))
plot(lm.fit2)
```

Here the residuals vs fitted values is much more aligned to what we desire to our ideal case, where variance of residuals is fairly constant. Also, the Normal Q-Q plot tells us the residuals distribution strongly approximates to the normal distribution as seen from the "nearly" straight line fit.

Model II :

It is good to check if there is any multi-collinearity between variables by analyzing the model using the vif function
```{r}
library(car)
vif(lm.fit2)                                      #vif function to assess collinearity
```
These are all resonably small, suggesting there is no potential collinearity present between them.

Model III :

Now from our initial plots, we noted that there were clear patterns between our casual variable and predictors like month, season,weekday and temp in particular. The first three clearly follow a non-linear relationship with the response variable, where there is a spike in the bike renters in the middle of the year and also the seasons.

Therefore we use the polynomial function to better fit these variables to our model response:  
 
```{r}
lm.fit3 <- lm(casuallog ~ year+poly(season,2)+temp+holiday+poly(weekday,2)+weathersit+poly(month,2)+hum+windspeed, data = train)
summary(lm.fit3)
```

The F-statistic shows there are significant variables responsible for prediction and the adjusted R squared is now close to 76 %. You also see that almost all the variables are significant as well.

Model IV :
Now if we plot the casual response vs temp predictor, which had the best correlation between among all the variables

```{r}
plot(train$casuallog ~ train$temp)
```

We observe the points tend to follow the curve of a cubic function (positive side).

Therefore, we can try modifying our temp predictor in our model to see if it yields any better responses 

```{r}
lm.fit4 <- lm(casuallog ~ year+poly(season,2)+poly(temp,3)+holiday+poly(weekday,2)+weathersit+poly(month,2)+hum+windspeed, data = train)

summary(lm.fit4)
```

The adjusted R squared is now 0.829! This means the model explains almost 83 % of the variability present in the data.Almost all predictor terms are significant as well, which indicates our response is affected by almost all of them.

Model V :

Season,temp,humidity and month all have a weak positive correlation between them. If you add an interaction term between season and temperature, you get : 

```{r}
lm.fit5 <- lm(casuallog ~ year+poly(season,2)+poly(temp,3)+holiday+poly(weekday,2)+weathersit+poly(month,2)+hum+season:temp+windspeed, data = train)
summary(lm.fit5)
```

You dont see any significant increase in the adjusted R squared value and also, we see the interaction effect is not significant as well(judging by the p-value). So we do not consider this interaction.

Model VI :

Now, we can try training our model using our original casual variable instead of the transformed response as it would be more easily interpretable.

```{r}
lm.fit6 <- lm(casual ~ year+poly(season,2)+poly(temp,3)+holiday+poly(weekday,2)+weathersit+poly(month,2)+hum+windspeed, data = train)
summary(lm.fit6)
```

Almost all variables are significant except for the higher order polynomial terms in season and the first order polynomial term in month. Also the adjusted R -squared has come down to 0.73.


Conclusion :

Final models :

T_Model I (Model IV above) with an adjusted R squared value of 0.829, we choose :
lm.fit4 <- lm(casuallog ~ year+poly(season,2)+poly(temp,3)+holiday+poly(weekday,2)+weathersit+poly(month,2)+hum+windspeed, data = train)

T_Model II (Model III above) with adjusted R squared value of 0.7588, we choose :

lm.fit3 <- lm(casuallog ~ year+poly(season,2)+temp+holiday+poly(weekday,2)+weathersit+poly(month,2)+hum+windspeed, data = train)

Please note: We didn't use the model with the interaction term even though it has a higher R squared value, since it is not contributing better to our original best fit model in any way.



II. Test
Using the test dataset (Test.csv) in this folder, perform the following analysis:
1. Use the two final models obtained in your training analysis to predict on the test data (hint: use predict(fitted model, test dataset) in R).

```{r}
test = read.csv("test.csv",header = TRUE)               #Read in test data set
str(test)
```

First, we need to create a new transformed log variable for the test data set to match our prediction values before re-converting it to casual

The statistic we use to judge which of our models is the best on a new data set is the Mean Squared value(MSE).
The smaller the test MSE, better our predictions (closer to actual test response).

Prediction using T_Model I:

```{r}
test$casuallog = log10(test$casual)               #Changing test response to match prediction
predictTest1 = predict(lm.fit4,newdata = test)    #predicting for test dataset using Model I
test$casualpredict1 = 10^(predictTest1)          #converting values back to original base of 10
MSE1 = (sum((test$casual - test$casualpredict1)^2))/nrow(test) #Mean squared error using modelI
paste0("Mean squared error using Training ModelI is ", round(MSE1,2) )
```

Prediction using T_Model II:

```{r}
predictTest2 = predict(lm.fit3,newdata = test)  #predicting for test dataset using Model II
test$casualpredict2 = 10^(predictTest2)         #converting values back to original base of 10 
MSE2 = (sum((test$casual - test$casualpredict2)^2))/nrow(test) #Mean squared error for modelII
paste0("Mean squared error using Training ModelII is ", round(MSE2,2) )
```

2. Compare predictions from the two models. Which model performs better? Why?

Model I is performing better than Model II.
Test MSE measures accuracy of the method in predicting the test data that are not used in training.The test MSE will be small if the predicted responses are very close to the true responses.
Comparing these two MSE's, we conclude that Model I is better because it is predicting our test responses much better than the second model as judged from the mean squared error statistic.

Plotting Predicted vs Actual values for our best fit model using ggplot, we get :

```{r}
library(ggplot2)
X <- data.frame(cbind(MI_predicted = test$casualpredict1,Actual = test$casual))
X
ggplot(X, aes(y = test$casualpredict1,x = test$casual))+geom_point(aes(color = "red"))+labs(title = "Predicted v Actual\n", x = "Actual Values", y = "Predicted Values", color = "Legend Title\n")
```


